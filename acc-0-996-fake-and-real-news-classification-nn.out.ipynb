{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fake and Real News Classification\n",
        "\n",
        "Dibya S. Barik ([Central University of\n",
        "Rajasthan](https://www.curaj.ac.in/))\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "# 1. üß† Objective\n",
        "\n",
        "The objective of this project is to build a **binary text classification\n",
        "model** to distinguish between **real** and **fake** news articles.  \n",
        "We use **word embeddings** and a **custom PyTorch neural network** to\n",
        "learn semantic representations from text and perform classification.\n",
        "\n",
        "> **Note**\n",
        ">\n",
        "> While this approach may be an **overkill for the small dataset**, it\n",
        "> provides an interesting and instructive example of building an\n",
        "> end-to-end NLP model using PyTorch.\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "# 2. üõ†Ô∏è Project Workflow\n",
        "\n",
        "1.  **üîΩ Data Loading**\n",
        "    -   Load datasets for real and fake news.\n",
        "    -   Assign binary labels: `1` for real, `0` for fake.\n",
        "2.  **üßπ Text Preprocessing**\n",
        "    -   Merge `title` and `text` fields into a `content` column.\n",
        "    -   Clean the text by removing stopwords and non-alphabetic tokens.\n",
        "3.  **üî§ Vocabulary & Tokenization**\n",
        "    -   Build a vocabulary from the training set.\n",
        "    -   Tokenize text and convert tokens into integer IDs.\n",
        "4.  **üì¶ Dataset and DataLoader Setup**\n",
        "    -   Define a custom `TextClassificationDataset` class.\n",
        "    -   Use `nn.EmbeddingBag` for efficient word embedding.\n",
        "    -   Create PyTorch `DataLoader`s for training and validation sets.\n",
        "5.  **üß† Model Architecture**\n",
        "    -   Define a feed-forward neural network with embeddings.\n",
        "    -   Train the model using cross-entropy loss and an optimizer like\n",
        "        Adam.\n",
        "6.  **üìà Training & Validation**\n",
        "    -   Train the model over multiple epochs.\n",
        "    -   Monitor performance using a validation set.\n",
        "7.  **üö´ No Test Evaluation**\n",
        "    -   This notebook focuses on training and validating the model.\n",
        "    -   No separate test set evaluation is performed.\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "# 3. üì¶ Importing Libraries\n",
        "\n",
        "We begin by importing the necessary libraries for:\n",
        "\n",
        "-   Tokenization (`tokenizers`, `nltk`)\n",
        "-   Dataset preparation (`torch`, `sklearn`)\n",
        "-   Model building (`torch.nn`)\n",
        "-   Data handling (`pandas`, `tqdm`)\n",
        "\n",
        "Warnings are suppressed for cleaner output."
      ],
      "id": "292a8935-31fa-4378-98e2-39eb18e0d51f"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "papermill": {
          "duration": 7.297321,
          "end_time": "2025-06-11T15:59:38.196368",
          "exception": false,
          "start_time": "2025-06-11T15:59:30.899047",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "id": "a64ffd20"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. üìÇ Loading Datasets\n",
        "\n",
        "We load two datasets: - `True.csv` for real news articles - `Fake.csv`\n",
        "for fake news articles\n",
        "\n",
        "We only select the `title` and `text` columns from each file."
      ],
      "id": "31ad11b3-ad52-417d-be01-49ae329bc1bb"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "papermill": {
          "duration": 2.517377,
          "end_time": "2025-06-11T15:59:40.729982",
          "exception": false,
          "start_time": "2025-06-11T15:59:38.212605",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "real = pd.read_csv('True.csv', usecols=['title', 'text'])\n",
        "fake = pd.read_csv('Fake.csv', usecols=['title', 'text'])"
      ],
      "id": "6361779c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.1 üëÄ Previewing Real News Data\n",
        "\n",
        "Let‚Äôs inspect the first few rows of the real news dataset to understand\n",
        "its structure and content."
      ],
      "id": "d8451371-8295-4ccd-bc8e-bde1e0a6138f"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "papermill": {
          "duration": 0.025685,
          "end_time": "2025-06-11T15:59:40.770814",
          "exception": false,
          "start_time": "2025-06-11T15:59:40.745129",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "\n",
              "</div>"
            ]
          }
        }
      ],
      "source": [
        "real.head()"
      ],
      "id": "1cbaabf8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1.1 üîç Observations:\n",
        "\n",
        "-   The dataset contains two textual fields: `title` and `text`.\n",
        "-   The texts are formal news content, often starting with a dateline\n",
        "    like ‚ÄúWASHINGTON (Reuters)‚Ä¶‚Äù. This structure will help guide our\n",
        "    preprocessing and tokenization strategy.\n",
        "\n",
        "## 4.2 üëÄ Previewing Fake News Data\n",
        "\n",
        "Let‚Äôs examine the first few rows of the fake news dataset to understand\n",
        "its structure and how it compares to the real news dataset.\n",
        "\n",
        "This helps verify consistency in formatting and content across both\n",
        "datasets."
      ],
      "id": "bfd4c296-097b-4f92-a62d-6c97992d6a1b"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "papermill": {
          "duration": 0.014211,
          "end_time": "2025-06-11T15:59:40.813968",
          "exception": false,
          "start_time": "2025-06-11T15:59:40.799757",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "\n",
              "</div>"
            ]
          }
        }
      ],
      "source": [
        "fake.head()"
      ],
      "id": "e7f95686"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Data Preparation\n",
        "\n",
        "## 5.1 üè∑Ô∏è Label Assignment\n",
        "\n",
        "To enable binary classification, we add a `label` column to each\n",
        "dataset:\n",
        "\n",
        "-   `1` for **real news**\n",
        "-   `0` for **fake news**"
      ],
      "id": "e476b915-f0e9-4fe0-a7f5-277ace97cf11"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "papermill": {
          "duration": 0.013092,
          "end_time": "2025-06-11T15:59:40.843809",
          "exception": false,
          "start_time": "2025-06-11T15:59:40.830717",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "real['label'] = 1\n",
        "fake['label'] = 0"
      ],
      "id": "fff5e0f3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.2 üß© Dataset Combination\n",
        "\n",
        "We concatenate both the labeled real and fake datasets into one unified\n",
        "DataFrame. This combined dataset will be used for training and\n",
        "evaluation."
      ],
      "id": "e8104f64-cee3-434f-ba1e-dc3c4067c6a9"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "papermill": {
          "duration": 0.012864,
          "end_time": "2025-06-11T15:59:40.873880",
          "exception": false,
          "start_time": "2025-06-11T15:59:40.861016",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "df = pd.concat([real, fake], ignore_index=True)"
      ],
      "id": "6fa5c1d1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.3 üî§ Preparing Stopwords\n",
        "\n",
        "We load a predefined set of common English stopwords using NLTK. These\n",
        "words typically provide little semantic value and can be removed during\n",
        "preprocessing."
      ],
      "id": "364d3aa5-a757-45a5-a658-f7b651eb1ab9"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "papermill": {
          "duration": 0.013106,
          "end_time": "2025-06-11T15:59:40.902399",
          "exception": false,
          "start_time": "2025-06-11T15:59:40.889293",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))"
      ],
      "id": "8b72fb68"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.4 üßπ Text Preprocessing Function\n",
        "\n",
        "We define a function `remove_stopwrods` that: - Tokenizes the input\n",
        "text - Removes stopwords and non-alphabetic tokens - Returns a cleaned\n",
        "version of the input text"
      ],
      "id": "62a6d4ce-c3b8-4394-8cab-028fb30472f5"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "papermill": {
          "duration": 0.010608,
          "end_time": "2025-06-11T15:59:40.928168",
          "exception": false,
          "start_time": "2025-06-11T15:59:40.917560",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(text):\n",
        "    word_tokens = word_tokenize(text)\n",
        "    filtered_text = [word for word in word_tokens if word.lower() not in stop_words and word.isalpha()]\n",
        "    return \" \".join(filtered_text)\n",
        "    "
      ],
      "id": "266c97de"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.5 üß± Rebuilding Unified Text Field\n",
        "\n",
        "We ensure that the `content` field is constructed by combining `title`\n",
        "and `text` again. This guarantees that any previous operations affecting\n",
        "`content` are overwritten and consistent."
      ],
      "id": "1db0eccc-32a8-45e0-b72c-a55164e6e11a"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "papermill": {
          "duration": 0.176272,
          "end_time": "2025-06-11T15:59:41.119921",
          "exception": false,
          "start_time": "2025-06-11T15:59:40.943649",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "df['content'] = df['title'] + ' ' + df['text']"
      ],
      "id": "467c15e5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.6 üßº Final Text Cleaning\n",
        "\n",
        "We apply our custom `remove_stopwrods` function to clean the combined\n",
        "`content` field. This includes removing stopwords and non-alphabetic\n",
        "tokens."
      ],
      "id": "609e43ad-daed-4249-bc9e-b31f6e45a941"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "papermill": {
          "duration": 90.171376,
          "end_time": "2025-06-11T16:01:11.307329",
          "exception": false,
          "start_time": "2025-06-11T15:59:41.135953",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "df['content'] = df['content'].apply(remove_stopwords)"
      ],
      "id": "53438e96"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.7 üîÄ Dataset Shuffling\n",
        "\n",
        "Shuffling the dataset ensures that there is no ordering bias when\n",
        "feeding data to the model."
      ],
      "id": "6d16d259-a1d7-40c1-9b3d-671b3b5cf21a"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "papermill": {
          "duration": 0.025351,
          "end_time": "2025-06-11T16:01:11.348751",
          "exception": false,
          "start_time": "2025-06-11T16:01:11.323400",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "df = df.sample(frac=1, random_state=101)"
      ],
      "id": "b766304a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.8 üìä Final Dataset Structure\n",
        "\n",
        "We keep only the `content` and `label` columns and reset the index. This\n",
        "creates a clean and consistent structure for downstream model training."
      ],
      "id": "96285ba9-a4f3-4c32-be3f-ff2d0fce4574"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "papermill": {
          "duration": 0.021299,
          "end_time": "2025-06-11T16:01:11.385336",
          "exception": false,
          "start_time": "2025-06-11T16:01:11.364037",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "\n",
              "</div>"
            ]
          }
        }
      ],
      "source": [
        "df = df[['content', 'label']].reset_index(drop=True)\n",
        "df.head()"
      ],
      "id": "368679ee"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.9 Preview Output\n",
        "\n",
        "From the preview, we confirm that each row has cleaned content and an\n",
        "associated binary label.\n",
        "\n",
        "# 6. üßæ DataFrame Overview\n",
        "\n",
        "We check the data type and memory usage of the dataset to confirm its\n",
        "readiness for model processing."
      ],
      "id": "2a99d4f6-ad7f-495d-a44b-5b3d5507d2f2"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "papermill": {
          "duration": 0.027582,
          "end_time": "2025-06-11T16:01:11.488953",
          "exception": false,
          "start_time": "2025-06-11T16:01:11.461371",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 44898 entries, 0 to 44897\n",
            "Data columns (total 2 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   content  44898 non-null  object\n",
            " 1   label    44898 non-null  int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 701.7+ KB"
          ]
        }
      ],
      "source": [
        "df.info()"
      ],
      "id": "d8e376d8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Data Overview Output**\n",
        ">\n",
        "> There are **44,898 samples** with two columns (`content`, `label`). No\n",
        "> missing values are present, confirming data integrity.\n",
        "\n",
        "# 7. üß± Custom Dataset Class\n",
        "\n",
        "This defines a `TextClassificationDataset` PyTorch class to:\n",
        "\n",
        "-   Split the dataset into train/test internally\n",
        "-   Tokenize the content using `tokenizers` library\n",
        "-   Return token IDs and labels in `__getitem__`\n",
        "\n",
        "This enables flexible loading and tokenization of samples."
      ],
      "id": "0f7aade1-e70c-4712-9695-a05f541ad8f0"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "papermill": {
          "duration": 0.013935,
          "end_time": "2025-06-11T16:01:11.529234",
          "exception": false,
          "start_time": "2025-06-11T16:01:11.515299",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "class TextClassificationDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer=None, text_column='content', label_column='label', train=False):\n",
        "        super().__init__()\n",
        "\n",
        "        train_df, test_df = train_test_split(dataframe,\n",
        "                                             test_size=0.1,\n",
        "                                             random_state=42,\n",
        "                                             shuffle=True,\n",
        "                                             stratify=dataframe[label_column]\n",
        "                                            )\n",
        "        \n",
        "        if train:\n",
        "            self.texts = train_df[text_column].tolist()\n",
        "            self.labels = train_df[label_column].tolist()\n",
        "\n",
        "            if tokenizer is None:\n",
        "                self._build_tokenizer(self.texts)\n",
        "            else:\n",
        "                self.tokenizer = tokenizer\n",
        "        else:\n",
        "            self.texts = test_df[text_column].tolist()\n",
        "            self.labels = test_df[label_column].tolist()\n",
        "\n",
        "            if tokenizer is None:\n",
        "                raise ValueError(\"Tokenizer must be provided for test split.\")\n",
        "\n",
        "            self.tokenizer = tokenizer\n",
        "        \n",
        "    def _build_tokenizer(self, texts):\n",
        "        self.tokenizer = Tokenizer(WordLevel(unk_token=\"<unk>\"))\n",
        "        self.tokenizer.pre_tokenizer = Whitespace()\n",
        "        trainer = WordLevelTrainer(special_tokens=[\"<unk>\", \"<pad>\"])\n",
        "        self.tokenizer.train_from_iterator(texts, trainer=trainer)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        token_ids = self.tokenizer.encode(self.texts[idx]).ids\n",
        "        token_ids = torch.tensor(token_ids, dtype=torch.long)\n",
        "        return token_ids, self.labels[idx]"
      ],
      "id": "f9406d63"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 8. üì¶ Batch Collation Function\n",
        "\n",
        "We define a `collate_batch` function to:\n",
        "\n",
        "-   Compute offsets for each sample (needed for `EmbeddingBag`)\n",
        "-   Return `token_ids`, `labels`, and `offsets` for batch processing"
      ],
      "id": "94487a09-11cb-4658-862c-905625637d21"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "papermill": {
          "duration": 0.011101,
          "end_time": "2025-06-11T16:01:11.556646",
          "exception": false,
          "start_time": "2025-06-11T16:01:11.545545",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def collate_batch(batch):\n",
        "    token_lists, labels = zip(*batch)\n",
        "\n",
        "    token_ids = torch.cat(token_lists)\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    offsets = [0]\n",
        "    for tokens in token_lists:\n",
        "        offsets.append(offsets[-1] + len(tokens))\n",
        "\n",
        "    offsets = torch.tensor(offsets[:-1], dtype=torch.long)\n",
        "    \n",
        "    return token_ids, labels, offsets"
      ],
      "id": "9eeadb20"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 9. üèóÔ∏è Initializing Datasets\n",
        "\n",
        "We instantiate training and validation dataset objects using our custom\n",
        "class. The tokenizer is trained only on the training set and reused for\n",
        "validation."
      ],
      "id": "004099b6-afc6-40c6-bd01-7521aebbbe92"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "papermill": {
          "duration": 3.777086,
          "end_time": "2025-06-11T16:01:15.349924",
          "exception": false,
          "start_time": "2025-06-11T16:01:11.572838",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "train_dataset = TextClassificationDataset(df, train=True)\n",
        "val_dataset = TextClassificationDataset(df, tokenizer=train_dataset.tokenizer)"
      ],
      "id": "fee4f656"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 10. üîÑ Dataloader Setup\n",
        "\n",
        "We create PyTorch DataLoaders for both training and validation datasets.\n",
        "Batch size and collation function are specified for efficient loading."
      ],
      "id": "d8016699-d8d3-4353-bafa-8e6b45a6fe23"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "papermill": {
          "duration": 0.010864,
          "end_time": "2025-06-11T16:01:15.377634",
          "exception": false,
          "start_time": "2025-06-11T16:01:15.366770",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=64, collate_fn=collate_batch, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, collate_fn=collate_batch)"
      ],
      "id": "9c54145c"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "papermill": {
          "duration": 0.160258,
          "end_time": "2025-06-11T16:01:15.543502",
          "exception": false,
          "start_time": "2025-06-11T16:01:15.383244",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "(tensor([  13,  866,   71,  ...,  575, 1455, 3816]),\n",
              " tensor([1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,\n",
              "         0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0]),\n",
              " tensor([    0,   156,   465,   704,  1073,  1518,  1714,  2001,  2162,  2612,\n",
              "          2657,  2871,  3048,  3358,  3377,  3544,  3788,  4101,  4157,  4346,\n",
              "          4876,  4947,  5058,  5328,  5469,  5486,  5506,  5618,  5833,  5853,\n",
              "          6045,  6269,  6400,  6545,  6563,  6735,  6883,  6935,  7019,  7316,\n",
              "          7562,  7872,  7972,  7981,  8034,  8250,  8397,  8782,  8879,  9701,\n",
              "          9928,  9945, 10179, 10303, 10542, 10753, 10805, 11028, 11091, 11599,\n",
              "         11609, 12232, 12450, 12813]))"
            ]
          }
        }
      ],
      "source": [
        "next(iter(train_loader))"
      ],
      "id": "2e40a905"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "papermill": {
          "duration": 0.020343,
          "end_time": "2025-06-11T16:01:15.569757",
          "exception": false,
          "start_time": "2025-06-11T16:01:15.549414",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "(tensor([11271,    87,   548,  ...,  4663,   138,   196]),\n",
              " tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1]),\n",
              " tensor([   0,  331,  449,  513,  884,  977, 1123, 1561, 1947, 2276, 2640, 2848,\n",
              "         3152, 3415, 3649, 3858]))"
            ]
          }
        }
      ],
      "source": [
        "next(iter(val_loader))"
      ],
      "id": "ff7d1432"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 11. Model Class"
      ],
      "id": "053d0faa-cfea-47a9-9f8a-28068f80eec2"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "papermill": {
          "duration": 0.011956,
          "end_time": "2025-06-11T16:01:15.587659",
          "exception": false,
          "start_time": "2025-06-11T16:01:15.575703",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size: int, embed_dim: int, num_classes: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        self.embedding = nn.EmbeddingBag(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
        "        self.fc = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        embed = self.embedding(text, offsets)\n",
        "        return self.fc(embed)"
      ],
      "id": "664f329f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 12. Model Training and Validation"
      ],
      "id": "9fc6239a-73bc-4fc9-875d-f3a4f3c71a45"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "papermill": {
          "duration": 93.759868,
          "end_time": "2025-06-11T16:02:49.353245",
          "exception": false,
          "start_time": "2025-06-11T16:01:15.593377",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epochs: 1/5] | train_loss: 0.20683, train_acc: 0.94370 | val_loss: 0.06314, val_acc: 0.98307\n",
            "[Epochs: 2/5] | train_loss: 0.04091, train_acc: 0.98963 | val_loss: 0.03342, val_acc: 0.99065\n",
            "[Epochs: 3/5] | train_loss: 0.01905, train_acc: 0.99611 | val_loss: 0.02316, val_acc: 0.99399\n",
            "[Epochs: 4/5] | train_loss: 0.01010, train_acc: 0.99817 | val_loss: 0.01761, val_acc: 0.99510\n",
            "[Epochs: 5/5] | train_loss: 0.00565, train_acc: 0.99911 | val_loss: 0.01415, val_acc: 0.99555"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "embed_dim = 200\n",
        "vocab_size = train_dataset.tokenizer.get_vocab_size()\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "model = TextClassifier(vocab_size, embed_dim, 2).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "losses = [] \n",
        "accs = []\n",
        "val_losses = []\n",
        "val_accs = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0.0\n",
        "    cur_len = 0\n",
        "    correct = 0\n",
        "\n",
        "    model.train()\n",
        "    for text, labels, offsets in train_loader:\n",
        "        text, labels, offsets = text.to(device), labels.to(device), offsets.to(device)\n",
        "        outputs = model(text, offsets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_size = labels.size(0)\n",
        "        total_loss += loss.item() * batch_size \n",
        "\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        cur_len += batch_size\n",
        "        \n",
        "\n",
        "    losses.append(total_loss/cur_len)\n",
        "    accs.append(correct/cur_len)\n",
        "\n",
        "    val_loss = 0.0\n",
        "    cur_len = 0\n",
        "    correct = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for text, labels, offsets in val_loader:\n",
        "            text, labels, offsets = text.to(device), labels.to(device), offsets.to(device)\n",
        "            outputs = model(text, offsets)\n",
        "    \n",
        "            loss = loss_fn(outputs, labels)\n",
        "    \n",
        "            batch_size = labels.size(0)\n",
        "            val_loss += loss.item() * batch_size \n",
        "    \n",
        "            _, predicted = torch.max(outputs, dim=1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            \n",
        "            cur_len += batch_size\n",
        "\n",
        "    val_loss = val_loss / cur_len\n",
        "    val_acc = correct / cur_len\n",
        "\n",
        "    val_losses.append(val_loss)\n",
        "    val_accs.append(val_acc)\n",
        "\n",
        "    print(f\"[Epochs: {epoch+1:>{len(str(epochs))}}/{epochs}]\",\n",
        "          f\"train_loss: {losses[-1]:.5f}, train_acc: {accs[-1]:.5f}\",\n",
        "          f\"val_loss: {val_losses[-1]:.5f}, val_acc: {val_accs[-1]:.5f}\",\n",
        "          sep=\" | \"\n",
        "         )"
      ],
      "id": "2611a111"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "papermill": {
          "duration": 0.433844,
          "end_time": "2025-06-11T16:02:49.798380",
          "exception": false,
          "start_time": "2025-06-11T16:02:49.364536",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax1.plot(range(1, epochs+1), losses, 'r-o', label='Train Loss')\n",
        "ax1.plot(range(1, epochs+1), val_losses, 'b-o', label='Val Loss')\n",
        "ax2.plot(range(1, epochs+1), accs, 'r-x', label='Train Accuracy')\n",
        "ax2.plot(range(1, epochs+1), val_accs, 'b-x', label='Val Accuracy')\n",
        "\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss', color='r')\n",
        "ax2.set_ylabel('Accuracy', color='b')\n",
        "\n",
        "ax1.legend(loc='center')\n",
        "ax2.legend(loc='center right')\n",
        "\n",
        "plt.title('Loss and Accuracy per Epoch')\n",
        "plt.show()"
      ],
      "id": "cell-fig-loss"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": "7278464",
          "sourceId": "11604520",
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": "31041",
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "papermill": {
      "default_parameters": {},
      "duration": "206.338435",
      "end_time": "2025-06-11T16:02:53.058424",
      "environment_variables": {},
      "exception": "",
      "input_path": "notebook.ipynb",
      "output_path": "notebook.ipynb",
      "parameters": {},
      "start_time": "2025-06-11T15:59:26.719989",
      "version": "2.6.0"
    }
  }
}